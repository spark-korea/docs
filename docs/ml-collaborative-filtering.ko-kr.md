# Collaborative Filtering / 협업필터링

협업 필터링은 일반적으로 추천 시스템에 사용된다. 

이러한 기술은 사용자 항목 연관 매트릭스의 누락된 항목을 채우는 것을 목표로 한다. 

spark.mlib는 현재 모델 기반 협업 필터링을 지원하며, 사용자와 제품은 누락된 항목을 예측하는 데 사용될 수 있는 작은 잠재 요인 집합에 의해 설명된다. 

spark.mlib는 Alternating Least Squares(ALS) 알고리즘을 사용하여 이러한 잠재 요인을 학습한다. 

spark.mlib의 구현에는 다음과 같은 매개 변수가 있다.

* numBlocks : 계산을 병렬화하는 데 사용되는 블록 수(자동 구성을 위해 -1이 기본값으로 설정)
* rank : 사용할 특징의 수(잠재 요인의 수)
* iterations : 실행할 ALS의 반복 횟수. ALS는 일반적으로 20회 이하의 반복으로 합리적인 솔루션으로 수렴된다.
* lambda : 정규화 매개변수를 ALS로 지정한다.
* implicitPrefs : 명시적 피드백 ALS 변형을 사용할지 또는 암시적 피드백 데이터에 맞게 조정된 변형을 사용할지 여부를 지정한다.
* alpha : 선호 관찰에 대한 기준 신뢰도를 지배하는 ALS의 암묵적 피드백 변형에 적용되는 매개 변수이다.

## 명시적 vs 암묵적 피드백
----------------------
매트릭스 인수 분해 기반 협업 필터링에 대한 표준 접근법은 사용자 항목 매트릭스의 항목을 사용자가 항목에 대해 제공하는 명시적 선호 사항으로 취급한다.

많은 실제 사용 사례에서 암묵적 피드백(예: 보기, 클릭, 구매, 좋아요, 공유)에만 액세스하는 것이 일반적이다. 이러한 데이터를 처리하기 위해 spark.mlib에 사용되는 접근 방식은 암시적 피드백 데이터 세트를 위한 협업 필터링에서 취한다.

 특히, 등급 매트릭스를 직접 모델링하려고 시도하는 대신, 이 접근법은 데이터를 사용자 동작 관찰의 강도(클릭 횟수 또는 누군가 영화를 보는 데 사용한 누적 지속시간 등)를 나타내는 숫자로 접근한다. 
 
 그러한 수치는 항목에 부여되는 명시적인 등급이 아니라 관찰된 이용자 선호도에 대한 신뢰 수준과 관련이 있다. 
 
 그런 다음, 모델은 항목에 대한 사용자의 예상 선호도를 예측하는 데 사용할 수 있는 잠재 요인을 찾고자 한다.

## 정규화 매개 변수의 스케일링
-------------------------
v1.1 이후, 우리는 사용자가 사용자 요인 업데이트에서 생성한 등급 수 또는 제품 요인 업데이트에서 제품이 받은 등급 수를 기준으로 각 최소 제곱 문제를 해결할 때 정규화 매개 변수 람다의 크기를 조정한다. 

이 접근법의 이름은 "ALS-WR"이며 "Netflix Prize를 위한 대규모 병렬 협업 필터링"이라는 논문에서 논의된다. 

람다는 데이터 세트의 규모에 덜 의존적이므로 샘플링된 하위 집합에서 학습된 최상의 매개 변수를 전체 데이터 집합에 적용할 수 있으며 유사한 성능을 기대할 수 있다.

## 예시
----------------------
다음 예에서는 등급 데이터를 로드합니다. 
각 행은 사용자, 제품 및 등급으로 구성됩니다. 등급이 명시적이라고 가정하는 기본 [ALS.train()]\(api/scala/org/apache/spark/mlib/recommendation/ALS$.html) 방법을 사용한다. 
우리는 등급 예측의 평균 제곱 오차(MSE)를 측정하여 권장 모델을 평가한다.
API에 대한 자세한 내용은 ALS Scala 문서를 참조하십시오.

{% include_example scala/org/apache/spark/examples/mllib/RecommendationExample.scala %}

등급 매트릭스가 다른 정보 근원(즉, 다른 신호에서 추론됨)에서 파생된 경우 trainImlicit 방법을 사용하여 더 나은 결과를 얻을 수 있다.

{% highlight scala %} val alpha = 0.01 val lambda = 0.01 val model = ALS.trainImplicit(ratings, rank, numIterations, lambda, alpha) {% endhighlight %}

MLlib의 모든 메소드들은 자바 친화적인 타입을 사용하므로 스칼라에서와 같은 방식으로 가져오고 호출할 수 있다. 
단 한가지 주의할 점은 이 메소드가 스칼라 RDD 객체를 사용하는 반면 스파크 자바 API는 별도의 자바RDD 클래스를 사용한다는 것이다. 
Java RDD는 'JavaRDD' 객체에서 '.rd()'를 호출하여 Scala로 변환할 수 있다. 
아래에 Scala에 제공된 예와 동일한 자체 포함 응용 프로그램 예가 나와 있다.
API에 대한 자세한 내용은 ALS Java 문서를 참조하십시오.

{% include_example java/org/apache/spark/examples/mllib/JavaRecommendationExample.java %}

다음 예에서는 정격 데이터를 로드한다. 
각 행은 사용자, 제품 및 등급으로 구성된다. 등급이 명시적이라고 가정하는 기본 ALS.train() 방법을 사용한다. 
우리는 등급 예측의 평균 제곱 오차(MSE)를 측정하여 권장 사항을 평가한다.
API에 대한 자세한 내용은 ALS Python 문서를 참조하십시오.

{% include_example python/mllib/recommendation_example.py %}

등급 매트릭스가 다른 정보 소스(즉, 다른 신호에서 추론됨)에서 파생된 경우 trainImlicit 방법을 사용하여 더 나은 결과를 얻을 수 있다.

{% highlight python %}


# 암시적 등급을 기반으로 ALS을 사용하여 권장 모델 작성
-------------------------
model = ALS.trainImplicit(ratings, rank, numIterations, alpha=0.01) {% endhighlight %}

위의 응용 프로그램을 실행하려면 스파크 빠른 시작 안내서의 자체 포함 응용 프로그램 섹션에 나와 있는 지침을 따르면 된다. 
또한 spark-mlib을 빌드 파일에 종속성으로 포함해야 한다.

## 튜토리얼
--------------
Spark Summit 2014의 교육 연습에는 spark.mllib과 함께 개인화된 영화 추천을 위한 실습 튜토리얼이 포함되어 있다.